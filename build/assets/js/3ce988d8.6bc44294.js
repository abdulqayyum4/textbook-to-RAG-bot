"use strict";(globalThis.webpackChunkai_textbook=globalThis.webpackChunkai_textbook||[]).push([[967],{8015:(n,e,o)=>{o.r(e),o.d(e,{assets:()=>r,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>l,toc:()=>c});var i=o(4848),t=o(8453);const s={},a="VLA Terminology and Glossary",l={id:"vla/terminology",title:"VLA Terminology and Glossary",description:"This document defines common terminology used throughout the Vision-Language-Action (VLA) educational content to ensure consistency across all modules.",source:"@site/docs/vla/terminology.md",sourceDirName:"vla",slug:"/vla/terminology",permalink:"/AI-Humanoid-Book/docs/vla/terminology",draft:!1,unlisted:!1,editUrl:"https://github.com/abdulqayyum4/AI-Humanoid-Book/edit/main/docs/docs/vla/terminology.md",tags:[],version:"current",frontMatter:{}},r={},c=[{value:"Key Terms",id:"key-terms",level:2},{value:"Vision-Language-Action (VLA) Systems",id:"vision-language-action-vla-systems",level:3},{value:"LLM-Robot Convergence",id:"llm-robot-convergence",level:3},{value:"Cognitive Planning",id:"cognitive-planning",level:3},{value:"Voice-to-Action Processing",id:"voice-to-action-processing",level:3},{value:"OpenAI Whisper",id:"openai-whisper",level:3},{value:"ROS 2 Action Sequences",id:"ros-2-action-sequences",level:3},{value:"Natural Language Understanding (NLU)",id:"natural-language-understanding-nlu",level:3},{value:"Task Decomposition",id:"task-decomposition",level:3},{value:"Vision Processing",id:"vision-processing",level:3},{value:"Navigation System",id:"navigation-system",level:3},{value:"Manipulation Control",id:"manipulation-control",level:3},{value:"Autonomous Humanoid",id:"autonomous-humanoid",level:3},{value:"System Integration",id:"system-integration",level:3},{value:"Acronyms",id:"acronyms",level:2}];function d(n){const e={h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"vla-terminology-and-glossary",children:"VLA Terminology and Glossary"}),"\n",(0,i.jsx)(e.p,{children:"This document defines common terminology used throughout the Vision-Language-Action (VLA) educational content to ensure consistency across all modules."}),"\n",(0,i.jsx)(e.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,i.jsx)(e.h3,{id:"vision-language-action-vla-systems",children:"Vision-Language-Action (VLA) Systems"}),"\n",(0,i.jsx)(e.p,{children:"Integrated systems that combine computer vision, natural language processing, and robotic action execution to enable robots to understand and respond to human commands."}),"\n",(0,i.jsx)(e.h3,{id:"llm-robot-convergence",children:"LLM-Robot Convergence"}),"\n",(0,i.jsx)(e.p,{children:"The integration of Large Language Models (LLMs) with robotic systems to enable natural language interaction and voice-to-action processing."}),"\n",(0,i.jsx)(e.h3,{id:"cognitive-planning",children:"Cognitive Planning"}),"\n",(0,i.jsx)(e.p,{children:"The system that translates high-level natural language commands into executable action sequences for robot control."}),"\n",(0,i.jsx)(e.h3,{id:"voice-to-action-processing",children:"Voice-to-Action Processing"}),"\n",(0,i.jsx)(e.p,{children:"The pipeline that converts voice commands to text, interprets the command, and generates robot actions."}),"\n",(0,i.jsx)(e.h3,{id:"openai-whisper",children:"OpenAI Whisper"}),"\n",(0,i.jsx)(e.p,{children:"A state-of-the-art speech recognition model used to convert voice commands to text for processing."}),"\n",(0,i.jsx)(e.h3,{id:"ros-2-action-sequences",children:"ROS 2 Action Sequences"}),"\n",(0,i.jsx)(e.p,{children:"Executable robot commands following ROS 2 standards for reliable, stateful command execution."}),"\n",(0,i.jsx)(e.h3,{id:"natural-language-understanding-nlu",children:"Natural Language Understanding (NLU)"}),"\n",(0,i.jsx)(e.p,{children:"The capability to interpret and comprehend human language commands in the context of robotic tasks."}),"\n",(0,i.jsx)(e.h3,{id:"task-decomposition",children:"Task Decomposition"}),"\n",(0,i.jsx)(e.p,{children:"The process of breaking down high-level commands into actionable steps that a robot can execute."}),"\n",(0,i.jsx)(e.h3,{id:"vision-processing",children:"Vision Processing"}),"\n",(0,i.jsx)(e.p,{children:"Computer vision components that enable the robot to perceive and understand its environment."}),"\n",(0,i.jsx)(e.h3,{id:"navigation-system",children:"Navigation System"}),"\n",(0,i.jsx)(e.p,{children:"Robot navigation capabilities for moving through environments safely."}),"\n",(0,i.jsx)(e.h3,{id:"manipulation-control",children:"Manipulation Control"}),"\n",(0,i.jsx)(e.p,{children:"Robot manipulation capabilities for interacting with objects."}),"\n",(0,i.jsx)(e.h3,{id:"autonomous-humanoid",children:"Autonomous Humanoid"}),"\n",(0,i.jsx)(e.p,{children:"A robot with human-like form and capabilities that can operate independently."}),"\n",(0,i.jsx)(e.h3,{id:"system-integration",children:"System Integration"}),"\n",(0,i.jsx)(e.p,{children:"The coordination and combination of vision, language, and action systems into a cohesive whole."}),"\n",(0,i.jsx)(e.h2,{id:"acronyms",children:"Acronyms"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"VLA"}),": Vision-Language-Action"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"LLM"}),": Large Language Model"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"NLU"}),": Natural Language Understanding"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"ROS"}),": Robot Operating System"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"API"}),": Application Programming Interface"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"AI"}),": Artificial Intelligence"]}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,o)=>{o.d(e,{R:()=>a,x:()=>l});var i=o(6540);const t={},s=i.createContext(t);function a(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);